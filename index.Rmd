---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Connor Hixon, cjh4572

### Introduction 

Last season, I recorded every play for UT basketball. These two data sets each include information regarding every offensive play Texas basketball had in 2020-21 season. I reduced the dataset down, however, so only Game #, opponent, play ID, poss#, Clutch/Garbage time, transition and 2nd chance possessions, number of passes, paint touch or not and point result were found. In the dataset, every game was given a game number found in the GAME.# column and there is also a column with the opponent we played. In the Play ID and Poss.# columns represent a number related to the possessions or play of the game which differ since there can be many plays on one possessions. On Clutch, Garbage, Transition and 2nd Chance, the columns represent a true false value for the specific variable. Paint Touch is also a true false value that represents whether the team had a paint touch that directly affected the outcome. Point Result represents the amount of points scored on the specific play.  

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
data1 <- read_csv('/stor/home/cjh4572/DATA/Dataset1.csv')
data2 <- read_csv('/stor/home/cjh4572/DATA/Dataset2.csv')

# if your dataset needs tidying, do so here
bball <- full_join(data1, data2, by = c('GAME.#', 'PLAY.ID', 'POSS.#'))
bball <- bball %>%
  select(`GAME.#`, OPPONENT, `POSS.#`, PLAY.ID, `CLUTCH?`, `GARBAGE?`, `TRANSITION?`,`2ND.CHANCE?`, PASSES, PAINT.TOUCH, POINT.RESULT)
```

### Cluster Analysis

```{R}
#Reducing the dataset to 4 numeric varibles
reduce_data <- bball %>%
  select(`POSS.#`,`PLAY.ID`, `POINT.RESULT`, `PASSES`)
library(cluster)
library(GGally)
library(ggplot2)
sil_width<-vector() 
for(i in 2:10){  
  kms <- kmeans(reduce_data,centers=i) #compute k-means solution for each k
  sil <- silhouette(kms$cluster,dist(reduce_data)) #get sil widths
  sil_width[i]<-mean(sil[,3]) #take averages (higher is better)
}
ggplot()+geom_line(aes(x=1:10,y=sil_width))+scale_x_continuous(name="k",breaks=1:10)

set.seed(160)
data_pam <- reduce_data %>% pam(k=2)
data_pam
plot(data_pam, which=2)


reduce_data %>%
  mutate(cluster = as.factor(data_pam$clustering)) %>%
  ggpairs(cols = 1:6, aes(color =cluster))
```

I created a subset of the dataset bball with variables, POSS.#, PLAY.ID, PASSES, POINT.RESULTS and called it reduce_data. When looking at the goodness of fit we can see that we get an average silhouette width of 0.62. This can be interpreted as a reasonable structure. When looking at the ggpair, the Poss.# and Play.ID have a very high correlation which should make sense since plays are correlated with the number of possessions that are occurring. This also means that Texas most of the time only had around 1 or 2 plays per possession. With the other variables in the data there is not much of a correlation such as there is not much of a correlation between how many passes in a possessions and the amount of points that occurs. 
    
    
### Dimensionality Reduction with PCA

```{R}
#get all of the numeric values and same to a new dataset
bball_n <- bball %>% select_if(is.numeric) %>% scale()
princomp(bball_n, cor=T) -> pca1
summary(pca1, loadings = T)

eigval <-  pca1$sdev^2 #square to convert SDs to eigenvalues
varprop=round(eigval/sum(eigval), 2) #proportion of var explained by each PC

ggplot() + geom_bar(aes(y=varprop, x=1:5), stat="identity") + xlab("") + geom_path(aes(y=varprop, x=1:5)) + 
  geom_text(aes(x=1:5, y=varprop, label=round(varprop, 2)), vjust=1, col="white", size=5) + 
  scale_y_continuous(breaks=seq(0, .6, .2), labels = scales::percent) + 
  scale_x_continuous(breaks=1:10)

```

For the PCA section, I created a new variable called bball_n which is the dataset that only contains the columns with numeric values from the bball dataset. When looking at the graph we can see that we get a PC1 value of 0.40 and a PC2 value of 0.2. In my data, PC3 has the same value has PC2 and PC4 is just below with a 0.19 value. It shows that around 99% of variance is shown through the PCA representing that there is not much correlation between the variables. 

###  Linear Classifier

```{R}
#reducing the dataset
n_data <- bball %>%
  select(`TRANSITION?`, POINT.RESULT, PASSES, `GAME.#`, PLAY.ID, `POSS.#`)

logistic_fit <- glm(`TRANSITION?` ~ ., data = n_data, family = 'binomial')
prob_reg <- predict (logistic_fit, type = 'response')

class_diag(prob_reg, bball$`TRANSITION?`, positive = "TRUE")


y = n_data$`TRANSITION?`
y <- factor(y, levels = c('TRUE', 'FALSE'))
y_hat <- sample(c('TRUE', 'FALSE'), size = length(y), replace = T)
table(actual = y, predicted = y_hat)
```

```{R}
#k fold CV
set.seed(1234)
k=10 #choose number of folds
data<-n_data[sample(nrow(n_data)),] #randomly order rows
folds<-cut(seq(1:nrow(n_data)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$`TRANSITION?` ## Truth labels for fold i
  ## Train model on training set (all but fold i)
  fit<-glm(as.factor(`TRANSITION?`)~.,data=train,family="binomial")
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test,type="response")
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags,mean)
```

When looking at the linear classifiers for the numeric variables and the binary variable `Transition?` (which represents whether a possessions was a transition possession),I used a logistic regression. When using the logistic regression, I got an auc of 0.720 and my k-fold auc represented a 0.7091. The auc value can be represented as a fair model. From the model, we can see that it has a very high specificity rate, those possessions without a transition. Over fitting is not shown in this model because the auc value decreased from the logistic regression and the cross-validation model. 

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(factor(`TRANSITION?`==1,levels=c("TRUE","FALSE")) ~ `PASSES` +`POINT.RESULT`+ `PLAY.ID`, data=n_data, k=5)
y_hat_knn <- predict(knn_fit,n_data)
head(y_hat_knn)


table(truth= factor(n_data$`TRANSITION?`==1, levels=c("TRUE","FALSE")),
      prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))

class_diag(y_hat_knn[,1],n_data$`TRANSITION?`, positive=1)
```

```{R}
# cross-validation of np classifier here
set.seed(1234)
k=10 #choose number of folds
data<-n_data[sample(nrow(n_data)),] #randomly order rows
folds<-cut(seq(1:nrow(n_data)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  ## Create training and test sets
  train<-data[folds!=i,] 
  test<-data[folds==i,]
  truth<-test$`TRANSITION?` ## Truth labels for fold i
  ## Train model on training set (all but fold i)
  fit<-knn3(as.factor(`TRANSITION?`)~ PASSES + `POINT.RESULT`  + `PLAY.ID` ,data=train)
  ## Test model on test set (fold i) 
  probs<-predict(fit,newdata = test)[,2]
  ## Get diagnostics for fold i
  diags<-rbind(diags,class_diag(probs,truth, positive=1))
}
summarize_all(diags,mean)
```

When looking at the kNN auc in class_diag() we get an auc value of 0.8676 which would fall in the good range for auc values. This is best modeling so far and when running the cross validation for the kNN, we get an auc value of 05993 which is a poor value for the cross validation. In the model, over fitting is not represented because auc value decreased significantly showing that it was not over fitting from the knn fit. When comparing the linear classifer and the non-parametric classifiers, the linear classifier was a much better cross-validation model to use. 

### Regression/Numeric Prediction

```{R}
fit<-lm(PASSES~.,data=bball) 
yhat<-predict(fit) 
mean((bball$PASSES-yhat)^2) 
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5 #choose number of folds
data<-bball[sample(nrow(bball)),] #randomly order rows
folds<-cut(seq(1:nrow(bball)),breaks=k,labels=F) #create folds
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  ## Fit linear regression model to training set
  fit<-lm(PASSES~.,data=train)
  ## Get predictions/y-hats on test set (fold i)
  yhat<-predict(fit,newdata=test)
  ## Compute prediction error  (MSE) for fold i
  diags<-mean((test$PASSES-yhat)^2) 
}
mean(diags)
```
When looking at the linear regression model and predicting the number of passes based on all of the variables in the bball dataset, we get an average MSE of 2.7628. When running the cross-validation regression model using a fit linear regression model, we get a MSE of 2.415 which is lower than the MSE value we got from the linear regression model. Therefore, since the cross-validation model was  smaller in MSE value than the linear regression line we can say the model does not represent over fitting. 

### Python 

```{R}
library(reticulate)
data <- bball %>%
  filter(`TRANSITION?` == 'TRUE')
```

```{python}
sum(r.data['PASSES'])
sum(r.data['PASSES'])/27
```

In the code, I used R coding to find all of the possessions that were transition possessions. Using the python code, I used the data found in R, to found that in all of the transition possessions Texas had a total of 144 passes and an average of 5.3 passes per game on transition possessions. 

### Concluding Remarks

When looking at the 2020-21 basketball season for UT basketball, it is hard to find many correlations and predictors when looking at the variables in our dataset. Through the ggpair clusters we were able to see how POSS.# and PLAY.ID were very correlated showing that usually there was around 1 or 2 plays per possession in the 2020-21 season. The other numeric values did not have too much of a correlation between each other. In the modeling part, we were able to use numeric predictors on the binary variable, transition, and found that the linear classifier cross-validation was the best model to use. When using python, we were able to see how to use the data from r to find the total number of passes and passes per game on transition possessions in python.  




